---
layout: post
title:  "Spectral Decompositions of Adjacency Matrices"
date:   2016-09-05
---

In this post, my goal is to establish why the adjacency matrix of a simple graph has its spectral decomposition; in my previous posts up until this point, I have relied on the fact that the adjacency matrix of a simple graph has its spectral decomposition, and related facts, without examining or justifying that these facts are true directly. Stated more precisely, my goal for this post is to justify that the adjacency matrix $$A$$ of any simple graph has the spectral decomposition $$A = \sum\limits_{\lambda \in \sigma(A)} \lambda E_\lambda$$, where $$\sigma(A)$$ denotes the set of distinct eigenvalues of $$A$$, and $$E_\lambda$$ denotes an orthogonal projection onto the eigenspace of $$\lambda$$ for all $$\lambda \in \sigma(A)$$. It turns out that the adjacency matrix of a simple graph has its spectral decomposition because it is a real symmetric matrix, and any real symmetric matrix has its spectral decomposition; so, I would like to focus somewhat more generally on establishing why any real symmetric matrix has a spectral decomposition, despite the spectral decomposition of the adjacency matrix of a simple graph being of particular relevance to the current subject matter of this blog.

Suppose that $$A$$ is a $$d$$ x $$d$$ real symmetric matrix. Then, in this context, I would propose that:

1. The minimal polynomial of $$A$$ splits into linear factors; moreover, each of its linear factors are distinct.

2. If the minimal polynomial $$p(x)$$ of $$A$$ splits into distinct linear factors as $$p(x) = \sum\limits_{k = 1}^{n} (x - \lambda_k)$$, then $$\operatorname{null}(A - \lambda_k I) = \operatorname{image}(q_k(A))$$ where $$q_k(x) := (x - \lambda_k)^{-1}p(x)$$ for all $$1 \leq k \leq n$$.

3. If the minimal polynomial $$p(x)$$ of $$A$$ splits into distinct linear factors as $$p(x) = \sum\limits_{k = 1}^{n} (x - \lambda_k)$$, then $$E_k$$ denotes an orthogonal projection onto the eigenspace of eigenvalue $$\lambda_k$$ of $$A$$ eigenvalue where $$E_k := (q_k(\lambda_k))^{-1} q_k(A)$$ and $$q_k(x) := (x - \lambda_k)^{-1}p(x)$$ for all $$1 \leq k \leq n$$; moreover, $$\sum\limits_{k=1}^{n} E_k = I$$.

Assuming that these three propositions hold in this context, then it is clear that $$A = A(I) = A \left(\sum\limits_{k=1}^{n} E_k \right) = \sum\limits_{k=1}^{n} A E_k = \sum\limits_{k=1}^{n} \lambda_k E_k$$, and hence has its spectral decomposition as desired.
I would now like to show that these three propositions, necessary to deduce that $$A$$ has its spectral decomposition, are indeed true in this context:

1. If $$A$$ is a real symmetric matrix, then it can also be interpreted as a Hermitian matrix whose entries all happen to be real. It turns out that for any Hermitian matrix, all of its eigenvalues are real. Hence, not only does the minimal polynomial of $$A$$ interpreted as a Hermitian matrix split into linear factors, as does the minimal polynomial of any matrix with entries in an algebraically closed field such as the field of complex numbers, but moreover all of its roots are real. Ultimately, since all of the roots of the minimal polynomial of $$A$$ interpreted as a Hermitian are real, then it is guaranteed that the minimal polynomial of $$A$$ viewed as a real symmetric matrix will split into linear factors as well (without needing to extend the field that $$A$$ takes entries in to include the complex numbers). Therefore, the minimal polynomial of $$A$$ splits into linear factors, as desired.

Now, suppose that $$\lambda$$ is any eigenvalue of $$A$$ such that$$(x - \lambda)^2$$ divides the minimal polynomial $$p(x)$$ of $$A$$, and that $$p(x) = (x - \lambda)^2 q(x)$$. Then, for any vector $$v \in \mathbb{R}^{d}$$ , $$\langle (A - \lambda I)^2 q(A)v, q(A)v \rangle = \langle (A - \lambda I)q(A)v, (A - \lambda I)q(A)v \rangle = 0$$ since $$A$$ is symmetric and hence self-adjoint; and, by properties of inner products, this implies$$(A - \lambda I)q(A)v = 0$$, which means that in fact $$p(x)$$ cannot be the minimal polynomial of $$A$$ as assumed since $$v$$ is arbitrary. It follows that there cannot be any eigenvalues of $$\lambda$$ of $$A$$ such that $$(x - \lambda)^{2}$$ divides the minimal polynomial of $$A$$. Hence, not only does the minimal polynomial of $$A$$ split into linear factors, but moreover all of the factors of the minimal polynomial of $$A$$ are distinct.

2. Assume that the minimal polynomial $$p(x)$$ of $$A$$ splits into distinct linear factors as $$p(x) = \sum\limits_{k = 1}^{n} (x - \lambda_k)$$, and select any particular $$k$$ where $$1 \leq k \leq n$$.

Firstly, select any eigenvector $$v \in \operatorname{null}(A - \lambda_k I)$$ for $$A$$ with eigenvalue $$\lambda_k$$. It turns out that since $$v$$ is an eigenvector for $$A$$, $$v$$ will be an eigenvector for any polynomial in $$A$$ as well; in particular, in this context it turns out that $$v$$ is an eigenvector for $$q_k(A)$$ where $$q_k(x) := (x - \lambda_k)^{-1}p(x)$$, since $$v$$ is an eigenvector for $$A$$ and $$q_k(A)$$ is a polynomial in $$A$$. Furthermore, in this context $$v = \sum\limits_{c = 1}^{d} a_c e_c$$ for coefficients $$a_c \in \mathbb{R}$$ for all $$1 \leq c \leq d$$, because $$v \in \operatorname{null}(A - \lambda_k I)$$ and $$\operatorname{null}(A - \lambda_k I)$$ is a subspace of $$\mathbb{R}^{d}$$. Hence $$q_k(A)\left ( \sum\limits_{c = 1}^{d} a_c e_c \right ) = \sum\limits_{c = 1}^{d} a_c \left ( q_k(A) e_c \right) = q_k(\lambda_k)v$$, which ultimately implies that $$v \in \operatorname{image}(q_k(A))$$ since $$\operatorname{image}(q_k(A)) = \operatorname{span}\left \{ q_k(A)e_c : 1 \leq c \leq d \right \}$$ and $$v = \sum\limits_{c = 1}^d (q_k(\lambda_k)^{-1} a_c \left( q_k(A)e_c \right )$$. Therefore $$\operatorname{null}(A - \lambda_k I) \subseteq \operatorname{image}(q_k(A))$$ since $$v$$ is arbitrary.

Now, select any vector $$w \in \operatorname{image}(q_k(A))$$ (where $$q_k(x) := (x - \lambda_k)^{-1}p(x)$$ as in the previous subcontext); this implies that $$w = q_k(A)x$$ for some $$x \in \mathbb{R}^{d}$$. Clearly, $$p(A)x = (A - \lambda_k I)q_k(A)x = 0$$, because $$p(x)$$ is the minimal polynomial of $$A$$. Since $$(A - \lambda_k I)\left( q_k(A)x \right ) = (A - \lambda_k I)w = 0$$, it follows that $$w \in \operatorname{null}(A - \lambda_k I)$$. Therefore $$\operatorname{image}(q_k(A)) \subseteq \operatorname{null}(A - \lambda_k I)$$ since $$w$$ is arbitrary.

Hence, $$\operatorname{null}(A - \lambda_k I) = \operatorname{image}(q_k(A))$$ as desired, since $$\operatorname{null}(A - \lambda_k I) \subseteq \operatorname{image}(q_k(A))$$ and $$\operatorname{image}(q_k(A)) \subseteq \operatorname{null}(A - \lambda_k I)$$. Therefore, it ultimately follows that $$\operatorname{null}(A - \lambda_j I) = \operatorname{image}(q_j(A))$$ for all $$1 \leq j \leq n$$ since $$k$$ is arbitrary as well.

3. As in the previous subcontext, assume that the minimal polynomial $$p(x)$$ of $$A$$ splits into distinct linear factors as $$p(x) = \sum\limits_{k = 1}^{n} (x - \lambda_k)$$, and select any particular $$k$$ where $$1 \leq k \leq n$$. Now, select any vector $$v \in \operatorname{null}(A - \lambda_k I) = \operatorname{image}(E_k)$$: it is straightforward to verify that $$E_k v = q_k(\lambda_k)^{-1} q_k(A) v = q_k(\lambda_k)^{-1} q_k(\lambda_k) v = v$$, and furthermore that $$E_k^{2} v = E_k (E_k v) = E_k v = v$$. It turns out that $$\operatorname{null}(E_k) = \operatorname{image}^{\perp}(E_k) = \operatorname{null}^{\perp}(A - \lambda_k I)$$, since eigenvectors with distinct eigenvalues are orthogonal for any real symmetric such as $$A$$; please see e.g. my Quora answer (http://1. How do I prove that eigenvectors corresponding to distinct eigenvalues of a real symmetric matrix are orthogonal? (https://www.quora.com/How-do-I-prove-that-eigenvectors-corresponding-to-distinct-eigenvalues-of-a-real-symmetric-matrix-are-orthogonal)) on the matter for a further justification of this fact. Hence $$E_k$$ is an orthogonal projection. Therefore, it ultimately follows that each $$E_j$$ is an orthogonal projection for all $$1 \leq j \leq n$$ as desired, since the choice of $$k$$ here is arbitrary.

Lastly, it turns out that the set of polynomials $$S = \left \{ q_k(\lambda_k))^{-1} q_k(x) : 1 \leq k \leq n \right \} $$is coprime. Since $$S$$ is a set of coprime polynomials in $$\mathbb{R}[x]$$, it follows from the analogue of BÃ©zout's identity for polynomials in $$\mathbb{R}[x]$$ that $$\sum\limits_{k=1}^{n} a_k(x) \left( q_k(\lambda_k))^{-1} q_k(x) \right) = 1$$ for some polynomials $$a_k(x) \in \mathbb{R}[x]$$ for all $$1 \leq k \leq n$$; this implies that $$\sum\limits_{k=1}^{n} a_k(A) \left( q_k(\lambda_k))^{-1} q_k(x) \right) = \sum\limits_{k=1}^{n} a_k(A) E_k = I$$. Moreover, we have that for all $$1 \leq j \leq n$$, $$E_j \left( \sum\limits_{k=1}^{n} a_k(A) E_k \right) = a_j(A) E_j^{2} = E_j$$; this implies that $$a_j(A) = 1$$ for $$1 \leq j \leq n$$ since each $$E_j$$ is idempotent. Therefore, it ultimately follows that $$\sum\limits_{k=1}^{n} E_k = I$$ as desired.
In conclusion, it follows that $$A$$ indeed has its spectral decomposition $$A = \sum\limits_{k = 1}^{n} \lambda_k E_k$$ as desired, since the three propositions needed to establish that $$A$$ has its spectral decomposition have been shown to hold in this context. Therefore, any real symmetric matrix has its spectral decomposition as desired, since $$A$$ arbitrary; and hence the adjacency matrix of any simple graph has its spectral decomposition as desired, since adjacency matrices of simple graphs are just special instances of real symmetric matrices.
